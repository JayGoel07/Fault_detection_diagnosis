{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hJv6BxKvrbh"
      },
      "outputs": [],
      "source": [
        "import os   #to acess listing files in folder\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder  # # Encoding categorical labels\n",
        "from tensorflow.keras import layers, models   # build N/N\n",
        "from tensorflow.keras.applications import MobileNet\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Data Aug\n",
        "from tensorflow.keras.regularizers import l2  # l2 reg.\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau #prevent overfitting & adjust lr\n",
        "from sklearn.metrics import classification_report  #evaluate model pred.\n",
        "\n",
        "# Function to load images from folder structure\n",
        "def load_images_from_folder(folder_path, label):\n",
        "    X, y = [], []   ## Lists to store images and labels\n",
        "    for filename in os.listdir(folder_path):\n",
        "        img_path = os.path.join(folder_path, filename)  # Get full file path\n",
        "        img = cv2.imread(img_path)\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, (224, 224))\n",
        "            img = img.astype('float32') / 255.0 # Normalize pixel values to range [0,1]\n",
        "            X.append(img)\n",
        "            y.append(label)\n",
        "    return np.array(X), np.array(y) # Convert lists to NumPy arrays and return\n",
        "\n",
        "# Function to train and evaluate the model using CNN (MobileNet)\n",
        "def train_and_evaluate_cnn(X_train, y_train, X_val, y_val):\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(y_train) #  Convert categorical labels to numerical values\n",
        "    y_val = label_encoder.transform(y_val)\n",
        "\n",
        "    # MobileNet as base model : Uses a pre-trained MobileNet model trained on ImageNet.\n",
        "    base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3)) #include_top=False → Removes the last classification layer.\n",
        "    base_model.trainable = False  # Freeze entire MobileNet model\n",
        "\n",
        "    # Create the CNN model\n",
        "    inputs = layers.Input(shape=(224, 224, 3))\n",
        "    x = base_model(inputs, training=False) #MobileNet as Feature extractor\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01))(x)\n",
        "    x = layers.BatchNormalization()(x)  # Stabilizes training\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.01))(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.GlobalAveragePooling2D()(x)  # Convert feature maps to a single vector\n",
        "    x = layers.Dense(256, activation='relu')(x) # Fully connected layer\n",
        "    x = layers.Dropout(0.6)(x)  #to reduce overfitting\n",
        "    outputs = layers.Dense(len(np.unique(y_train)), activation='softmax')(x) #o/p layer\n",
        "\n",
        "    model = models.Model(inputs, outputs)   #Adam optimisation\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.4,\n",
        "        height_shift_range=0.4,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.3,\n",
        "        brightness_range=[0.8, 1.2],\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Early Stopping & Learning Rate Scheduler\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True) # Stops training if validation loss doesn’t improve for 5 epochs\n",
        "    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "                        epochs=30,  # Increased to allow early stopping\n",
        "                        validation_data=(X_val, y_val),\n",
        "                        callbacks=[early_stopping, lr_scheduler])\n",
        "\n",
        "    # Evaluate the model\n",
        "    val_loss, val_accuracy = model.evaluate(X_val, y_val)\n",
        "    print(f\"Validation Accuracy: {val_accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Classification report\n",
        "    y_val_pred = np.argmax(model.predict(X_val), axis=1)\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_val, y_val_pred))\n",
        "\n",
        "    return history, val_accuracy\n",
        "\n",
        "\n",
        "# Define folder paths for each class\n",
        "folder_paths = {\n",
        "    'fault_A': '/content/drive/MyDrive/btp_dataset_folder/btp_dataset/faulta_noise_15dB',\n",
        "    'fault_B': '/content/drive/MyDrive/btp_dataset_folder/btp_dataset/faultb_noise_15dB',\n",
        "    'fault_C': '/content/drive/MyDrive/btp_dataset_folder/btp_dataset/faultc_noise_15dB',\n",
        "    'healthy': '/content/drive/MyDrive/btp_dataset_folder/btp_dataset/healthy_noise_15dB',\n",
        "}\n",
        "\n",
        "# Initialize lists for training and validation data\n",
        "X_train_total, y_train_total, X_val_total, y_val_total = [], [], [], []\n",
        "\n",
        "# Loop to perform 70/30 split for each folder\n",
        "for class_label, folder_path in folder_paths.items():\n",
        "    X, y = load_images_from_folder(folder_path, class_label)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "    X_train_total.append(X_train)\n",
        "    y_train_total.append(y_train)\n",
        "    X_val_total.append(X_val)\n",
        "    y_val_total.append(y_val)\n",
        "\n",
        "# Combine all 30% validation data from all classes\n",
        "X_val_total = np.concatenate(X_val_total)\n",
        "y_val_total = np.concatenate(y_val_total)\n",
        "\n",
        "# Now train using 70% data from each class and 30% mixed validation data\n",
        "X_train_total = np.concatenate(X_train_total)\n",
        "y_train_total = np.concatenate(y_train_total)\n",
        "\n",
        "# Train and evaluate the CNN model\n",
        "history, val_accuracy = train_and_evaluate_cnn(X_train_total, y_train_total, X_val_total, y_val_total)\n",
        "\n",
        "print(f\"\\nFinal validation accuracy: {val_accuracy * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCCsz3Fawxth",
        "outputId": "f6a81fcf-ec24-457b-f093-024a7bf0db07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}